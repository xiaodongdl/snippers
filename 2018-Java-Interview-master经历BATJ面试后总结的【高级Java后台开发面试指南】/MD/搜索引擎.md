搜索引擎是因为个人项目关系，那个搜索引擎非常简单，但有很多东西可以问到，可选择性学习借鉴，了解到对项目是如何提问的，还有问题的深度

![](https://github.com/xbox1994/2018-Java-Interview/raw/master/images/j10.png)

* 网页采集
* 内容过滤
* 信息存储
* 信息检索与后台管理

## 爬虫
### 原理
仅仅针对网页的源代码进行下载，不包含信息结构化提取

* 下载工具：使用HttlClient下载，放入Vector，用ConcurrentHashMap做爬虫深度控制
* URL解析逻辑：将网页源代码中的URL过滤出来放入集合

首先在下载队列中，需要安排一些源URL，当下载工具把网页源码下载下来之后，从中解析出能继续当成源URL的URL放入下载集合中，直到下载工具把下载集合中的内容都下载完成

### 为什么用HttpClient为下载工具
**简单易用，符合需求场景，不需要更高更强的功能**

HttpURLConnection：本身的 API 不够友好，所提供的功能也有限  
HttpClient：功能强大  
OkHttp：是一个专注于性能和易用性的 HTTP 客户端。OkHttp 会使用连接池来复用连接以提高效率。OkHttp 提供了对 GZIP 的默认支持来降低传输内容的大小。OkHttp 也提供了对 HTTP 响应的缓存机制，可以避免不必要的网络请求。当网络出现问题时，OkHttp 会自动重试一个主机的多个 IP 地址

### 如何保存URL
封装一个包含同步的添加、移除方法LinkedList来作为存储URL的集合

### 如何做爬虫深度控制
用ConcurrentHashMap保存【URL -> 深度】的键值对，在爬虫下载的过程中同时记录下深度，所以需要通过多个线程高频地对这个键值对集合进行操作，还要在增加元素的时候需要进行同步操作以免元素被覆盖，还需要避免多线程reset导致的循环依赖问题

### URL如何解析并保存
在下载到网页源码之后，将其中的所有URL通过Jsoup或者对每行内容进行正则表达式提取出来，如果符合预设值的正则表达式就添加到下载集合中。

### 多线程爬虫如何实现
使用Executors.newFixedThreadPool()，因为我爬虫线程基本固定不会需要销毁重建，并且可以控制线程最大数量，方便日后根据机器性能调整。

如果问到线程池的原理，请参[这里](http://www.wangtianyi.top/blog/2018/05/08/javagao-bing-fa-wu-xian-cheng-chi/?utm_source=github&utm_medium=github)。

### 抵抗反爬虫策略
#### 动态页面的加载
phantomjs + selenium

#### 如何抓取需要登录的页面
模拟登录之后将sessionId保存到request header的cookie中

#### 如何解决IP限制问题
买个支持ADSL的拨号服务器，便宜的一个月80


#### 每分钟访问频率
寻找网站访问频率访问限制漏洞，自己探索不同网站的访问频率限制规则

#### 验证码
图像识别

### 如何实现数据库连接池
1. 连接池的建立。使用外部配置的连接池信息初始化一定数量的连接，结合CopyOnWriteArrayList来存储连接
2. 连接池的管理。当客户请求数据库连接时，从ThreadLocal或线程共享的连接池查看是否有空闲连接，如果存在空闲连接，则将连接的引用借给客户使用；如果没有空闲连接，则查看当前所开的连接数是否已经达到最大连接数，如果没达到就重新创建一个连接给请求的客户；如果达到就按设定的最大等待时间进行等待，如果超出最大等待时间，则抛出异常给客户。
3. 连接池的关闭。当应用程序退出时，关闭连接池中所有的连接，释放连接池相关的资源，该过程正好与创建相反。

#### [并发、性能问题](http://0x0010.com/2017/09/performance-promote-of-using-threadlocal-in-hikaricp/)
使用synchronized、lock可解决多线程分发与回收线程问题。

但性能问题还是很大，主要是因为**加锁、释放锁、挂起、恢复线程的上下文切换的开销**，所以为了提高性能，可进行以下方面的优化：  
避免同步操作，更换为其他操作。使用**ThreadLocal**将线程池的副本保存在每个线程中来减少锁竞争，同时结合**CAS乐观锁**处理连接的分发与回收

#### 增加连接超时功能
参考HikariCP中CurrentBag中borrow方法：

```
timeout = timeUnit.toNanos(timeout);
 do {
    final long start = currentTime();
    // 在一定时间内，阻塞地从SynchronousQueue中试图获取生产好的连接实例
    // 如果队列中没有可以用的连接实例，返回null
    // 如果队列中有可以用的实例，那么把状态改成USE并且返回
    final T bagEntry = handoffQueue.poll(timeout, NANOSECONDS);
    if (bagEntry == null || bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_IN_USE)) {
       return bagEntry;
    }
	 
	 // timeout循环被减小，直到大于0.01毫秒
    timeout -= elapsedNanos(start);
 } while (timeout > 10_000);

```

### TF-IDF
#### 原理
如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

一个词在本文出现次数/本文总词数  
（文章总数/这个词在所有文章中出现次数）取对数  
 相乘

#### 目的
去掉网页内容相同或相似的网页

#### 用法
用TF-IDF（term frequency–inverse document frequency）统计每个文章中少数重要词（排序后的前几个），用MD5算法把每个词算出一个16进制的数全部加在一起然后比较两个总数，如果相同，那么两篇文章中重要的词就相同的，去掉。

## Lucene原理
### 搜索原理
词项查询(TermQuery)  
布尔查询(BooleanQuery)  
短语查询(PhraseQuery)  
范围查询(RangeQuery)  
百搭查询(WildardQuery)  
FuzzQuery(模糊)

### 倒排索引
不是由记录来确定属性值，而是由属性值来确定记录的位置。

#### 构建过程

1. Analyzer分词
2. 根据单词生成索引表，同时得到“词典文件”（词-> 单词ID）
3. 得到“倒排列表”（单词ID -> 文档ID）

设有两篇文章1和2   
文章1的内容为：Tom lives in Guangzhou,I live in Guangzhou too.
文章2的内容为：He once lived in Shanghai.

|关键词|文章号|出现频率|出现位置|
|---|---|---|---|
|guangzhou|1|2|3,6|
|live|1|2|2,5|
|live|2|1|2|
|shanghai|2|1|3|
|tom|1|1|1|

Lucene将上面三列分别作为词典文件、频率文件、位置文件保存。其中词典文件不仅保存有每个关键词，还保留了指向频率文件和位置文件的指针，通过指针可以找到该关键字的频率信息和位置信息。

#### 词典文件（HashMap）

![](https://github.com/xbox1994/2018-Java-Interview/raw/master/images/j7.jpg)

#### 倒排表

![](https://github.com/xbox1994/2018-Java-Interview/raw/master/images/j8.gif)

欢迎光临[91Code](http://www.91code.info/?utm_source=github&utm_medium=github)，发现更多技术资源~
